![CI Status](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml/badge.svg)](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml)

# API App Stocks â€“ Data Pipeline con Python, Airflow y Redshift

## ğŸ“Œ DescripciÃ³n general

Este proyecto implementa un pipeline de datos que:

- Obtiene diariamente precios de acciones desde la API de Yahoo Finance (`yfinance`).
- Aplica transformaciones explÃ­citas sobre los datos (normalizaciÃ³n, casting y categorizaciÃ³n).
- Guarda el histÃ³rico en formato **CSV** y **Parquet**.
- Carga los datos transformados a **Amazon Redshift** usando SQLAlchemy.
- Orquesta todo con **Apache Airflow** corriendo en Docker.
- Asegura calidad con **tests unitarios** y **CI en GitHub Actions**.

---

## ğŸ—ï¸ Arquitectura del pipeline

```text
            +----------------+
            |  yfinance API  |
            +--------+-------+
                     |
                     v
         +-----------------------+
         |  ExtracciÃ³n (Python)  |
         |  get_stock_data()     |
         +-----------+-----------+
                     |
                     v
      +--------------------------------+
      | TransformaciÃ³n (Python)        |
      | transform_stock_data()         |
      | - Normaliza Ticker             |
      | - Price float â†’ int            |
      | - Price_Bucket (categorÃ­as)    |
      +--------------+-----------------+
                     |
                     v
     +--------------------------------------+
     | Persistencia local                   |
     | - CSV histÃ³rico                      |
     | - Parquet (data/staging)             |
     +----------------+---------------------+
                     |
                     v
         +------------------------------+
         |   Carga a Redshift (Python)  |
         | load_parquet_to_redshift()   |
         +--------------+---------------+
                     |
                     v
            +-------------------+
            |   Data Warehouse  |
            |     Redshift      |
            +-------------------+

---

## ğŸ”§ Transformaciones de datos

La funciÃ³n `transform_stock_data(df)` aplica transformaciones de negocio sobre el DataFrame de precios:

- **Ticker** â†’ convertido a mayÃºsculas.
- **Price** â†’ preservado como **float** (tal cual viene desde la fuente).
- **Price_Bucket** calculado segÃºn rangos:
  - LOW â†’ â‰¤ 100  
  - MEDIUM â†’ 100â€“500  
  - HIGH â†’ > 500

**Importante:**  
No se fuerza la conversiÃ³n a `int` en el pipeline.  
Los tests unitarios validan que **se puede convertir** sin modificar el cÃ³digo productivo.

Adicionalmente:

- EliminaciÃ³n de duplicados en el histÃ³rico por (`Date`, `Ticker`).
- Persistencia en CSV + Parquet.

---

## ğŸ§ª Tests unitarios

Los tests incluidos verifican:

Incluyen:

### âœ” Test con mock de yfinance
- Simula la API sin hacer requests reales.
- Verifica que:
  - el DataFrame se produce correctamente,
  - `Price` es float,
  - puede convertirse a `int` si se necesitara.

### âœ” Test para transformaciones
- Verifica:
  - normalizaciÃ³n del ticker,
  - tipo float de Price,
  - bucketizaciÃ³n correcta.

### Ejecutar tests:

```bash
pytest -q

---

ğŸ—„ï¸ Carga a Redshift

`redshift_loader.py`:

- Lee credenciales desde `.env`.
- Crea engine SQLAlchemy.
- Lee Parquet: `data/staging/stock_prices_history.parquet`.
- Ejecuta `to_sql()` hacia Redshift.

Variables necesarias:


REDSHIFT_HOST=
REDSHIFT_PORT=
REDSHIFT_USER=
REDSHIFT_PASSWORD=
REDSHIFT_DB=
REDSHIFT_SCHEMA=
REDSHIFT_TABLE=
PARQUET_PATH=data/staging/stock_prices_history.parquet


---

 ğŸŒ¬ï¸ DAG de Airflow

`dags/stocks_redshift_daily_dag.py`

Tareas:

1. `fetch_stocks_daily`
2. `load_to_redshift`

Flujo:

```
fetch_stocks_daily >> load_to_redshift
```

Escribe CSV y Parquet en:

```
data/stock_prices_history.csv
data/staging/stock_prices_history.parquet
```

---

## ğŸ“‚ Estructura del proyecto

```
Api_app_Stocks/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ constants.py
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ get_data.py
â”‚       â”œâ”€â”€ redshift_loader.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ stocks_redshift_daily_dag.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_get_stock_data.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ tests.yml
â”œâ”€â”€ Dockerfile.airflow
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ’» EjecuciÃ³n local (Python)

```
uv sync
uv run python -m app.src.get_data
uv run python -m app.src.redshift_loader
```

---

## ğŸ³ EjecuciÃ³n con Docker + Airflow

### 1. Crear archivo `.env`

```
cp .env.example .env
```

### 2. Construir la imagen:

```
docker compose build
```

### 3. Inicializar Airflow:

## ğŸ§¾ Atajos con Makefile para Airflow

Para simplificar la ejecuciÃ³n de Docker + Airflow, el proyecto incluye un `Makefile` con comandos de ayuda.

### Comandos disponibles

```bash
# Construir la imagen de Airflow (Dockerfile.airflow)
make airflow-build

# Inicializar la base de datos de Airflow
make airflow-init

# Crear usuario administrador de Airflow (admin / admin)
make airflow-create-user

# Levantar Airflow (webserver + scheduler + postgres)
make airflow-up

# Bajar todos los servicios de Airflow
make airflow-down

# Reset completo de Airflow:
# - baja servicios
# - borra volÃºmenes
# - build de imagen
# - init de DB
# - crea usuario admin
make airflow-reset

Flujo para levantar Airflow desde cero:

make airflow-build
make airflow-init
make airflow-create-user
make airflow-up

Luego, acceder a:

UI: http://localhost:8080

Usuario: admin

Password: admin

```
docker compose up airflow-init
```

### 4. Crear usuario admin:

```
docker compose run --rm airflow-webserver airflow users create   --username admin   --firstname Admin   --lastname User   --role Admin   --email admin@example.com   --password admin
```

### 5. Levantar Airflow:

```
docker compose up
```

UI: http://localhost:8080

Login: `admin / admin`

### 6. Activar y correr el DAG

1. Activar toggle del DAG  
2. "Trigger DAG"  
3. Revisar logs de `fetch_stocks_daily` y `load_to_redshift`.

---

## ğŸ” Manejo de credenciales

`.env` debe estar en `.gitignore`  
`.env.example` solo contiene placeholders.

---

## âœ… Resumen de comandos

```
pytest -q
uv sync
docker compose build
docker compose up airflow-init
docker compose run --rm airflow-webserver airflow users create ...
docker compose up
docker compose down
```

