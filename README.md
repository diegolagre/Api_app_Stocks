[![CI Status](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml/badge.svg)](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml)

[![CI Status](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml/badge.svg)](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml)

# API App Stocks â€“ Data Pipeline con Python, Airflow y Redshift

## ğŸ“Œ ABSTRACT

Este Trabajo PrÃ¡ctico implementa un **pipeline de ingenierÃ­a de datos moderno y automatizado**, compuesto por:

- **Ingesta diaria** de datos de acciones desde la API de Yahoo Finance (yfinance)  
- **Transformaciones explÃ­citas** (normalizaciÃ³n, casting, categorizaciÃ³n)  
- Persistencia en **CSV** y **Parquet (staging)**  
- **Carga incremental a Amazon Redshift**  
- **OrquestaciÃ³n completa con Apache Airflow** (incluye backfill)  
- **Calidad garantizada con tests unitarios + mocking**  
- IntegraciÃ³n continua con **GitHub Actions (CI)**  

El pipeline cumple todos los requisitos del TP:
- Fuente de datos externa  
- Transformaciones explÃ­citas  
- DW (Redshift)  
- Tests manuales + unitarios  
- OrquestaciÃ³n en Airflow  
- Flujo reproducible y automatizable  

---

# ğŸ—ï¸ ARQUITECTURA GENERAL DEL PIPELINE

        +----------------+
        |  yfinance API  |
        +--------+-------+
                 |
                 v
     +-----------------------+
     |  ExtracciÃ³n (Python)  |
     |  get_stock_data()     |
     +-----------+-----------+
                 |
                 v
  +--------------------------------+
  | TransformaciÃ³n (Python)        |
  | transform_stock_data()         |
  | - Normaliza Ticker             |
  | - Price float â†’ int            |
  | - Price_Bucket (categorÃ­as)    |
  +--------------+-----------------+
                 |
                 v
 +--------------------------------------+
 | Persistencia local                   |
 | - CSV histÃ³rico                      |
 | - Parquet staging (data/staging)     |
 +----------------+----------------------+
                 |
                 v
     +------------------------------+
     |   Carga a Redshift (Python)  |
     | load_parquet_to_redshift()   |
     +--------------+---------------+
                 |
                 v
        +-------------------+
        |   Data Warehouse  |
        |     Redshift      |
        +-------------------+


---

# ğŸ”§ TRANSFORMACIONES IMPLEMENTADAS

La transformaciÃ³n principal se realiza en **transform_stock_data(df)**:

### âœ” NormalizaciÃ³n de datos
- `Ticker` â†’ **mayÃºsculas**
- `Price` â†’ **entero seguro** (cast with coercion)

### âœ” CreaciÃ³n de columna derivada (categorizaciÃ³n)
`Price_Bucket`:
- `LOW` â†’ precios â‰¤ 100  
- `MEDIUM` â†’ 100 < precio â‰¤ 500  
- `HIGH` â†’ precio > 500  

### âœ” Limpieza
- Forzar tipos  
- Manejo de nulos  
- UnificaciÃ³n histÃ³rico sin duplicados (`Date`, `Ticker`)  



---

# ğŸ§ª TESTS UNITARIOS

Los tests estÃ¡n en `tests/test_get_stock_data.py` e incluyen:

### ğŸŸ¢ Test 1 â€“ Mock de yfinance  
Valida:
- no se llama a la API real  
- conversiÃ³n Price float â†’ int  
- DataFrame generado correctamente  

### ğŸŸ¢ Test 2 â€“ IntegraciÃ³n bÃ¡sica  
Verifica columna, tipos y estructura.

### ğŸŸ¢ Test 3 â€“ transform_stock_data  
Valida que las transformaciones funcionen:

- uppercase  
- Price int  
- Price_Bucket correcto  
- estructura final

GitHub Actions ejecuta automÃ¡ticamente todos los tests en cada push.

---

# ğŸ§­ DAG DE AIRFLOW

El DAG principal es:

`dags/stocks_redshift_daily_dag.py`

Tareas:

### 1ï¸âƒ£ fetch_stocks_daily
- Obtiene datos  
- Aplica transformaciones  
- Genera CSV  
- Genera Parquet en `/opt/airflow/data/staging/`

### 2ï¸âƒ£ load_to_redshift
- Lee el parquet  
- Inserta en Redshift usando SQLAlchemy  

Flujo:


Se ejecuta diariamente a las 10:00 UTC.

---

# ğŸ—„ï¸ CARGA A REDSHIFT

`app/src/redshift_loader.py`:

- Construye un engine con SQLAlchemy desde variables de entorno  
- Lee el Parquet  
- Inserta los datos en Redshift mediante `df.to_sql()`  
- Maneja creaciÃ³n del schema/tablas segÃºn config  

Variables que deben estar presentes (.env):

REDSHIFT_HOST=
REDSHIFT_PORT=5439
REDSHIFT_USER=
REDSHIFT_PASSWORD=
REDSHIFT_DB=
REDSHIFT_SCHEMA=public
REDSHIFT_TABLE=stock_prices_history


---

# ğŸ“‚ ESTRUCTURA DEL PROYECTO

Api_app_Stocks/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ constants.py
â”‚ â””â”€â”€ src/
â”‚ â”œâ”€â”€ get_data.py
â”‚ â”œâ”€â”€ redshift_loader.py
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ stocks_redshift_daily_dag.py
â”‚
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ test_get_stock_data.py
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ staging/
â”‚ â””â”€â”€ stock_prices_history.parquet
â”‚
â”œâ”€â”€ .github/workflows/tests.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md


---

# ğŸš€ CÃ“MO EJECUTAR EL PROYECTO â€“ PASO A PASO

A continuaciÃ³n tenÃ©s **todo el walkthrough completo**, tal como lo pedirÃ­a un evaluador del TP.

---

## 1ï¸âƒ£ Clonar el repositorio

```bash
git clone https://github.com/diegolagre/Api_app_Stocks.git
cd Api_app_Stocks



##2ï¸âƒ£ Configurar entorno con uv
uv sync

##3ï¸âƒ£ Crear .env

cp .env.example .env
#CompletÃ¡ con tus credenciales reales de Redshift.

##4ï¸âƒ£ Ejecutar solo la EXTRACCIÃ“N (opcional)

uv run python -m app.src.get_data

Esto genera:

stock_prices_history.csv
data/staging/stock_prices_history.parquet

##5ï¸âƒ£ Ejecutar solo la CARGA a Redshift (opcional)

uv run python -m app.src.redshift_loader

##6ï¸âƒ£ Ejecutar TESTS UNITARIOS

pytest -q

##7ï¸âƒ£ Levantar Airflow con Docker (si corresponde en tu entorno)

docker compose up airflow-init
docker compose up

#Abrir:

#ğŸ‘‰ http://localhost:8080

#Activar DAG:

#ğŸ‘‰ stocks_redshift_daily_dag

#El pipeline descargarÃ¡ datos â†’ transformarÃ¡ â†’ generarÃ¡ staging â†’ cargarÃ¡ Redshift.

##8ï¸âƒ£ Verificar los datos en Redshift

SELECT * 
FROM public.stock_prices_history
ORDER BY date DESC, ticker;
