[![CI Status](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml/badge.svg)](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml)

# ğŸš€ API App Stocks  
### *Data Pipeline con Python, Airflow, Docker y Redshift*

---

# ğŸ“˜ 1. DescripciÃ³n general

Este proyecto implementa un **pipeline de ingenierÃ­a de datos** capaz de:

âœ… Extraer precios diarios de acciones desde **Yahoo Finance (yfinance)**  
âœ… Transformar datos (normalizaciÃ³n + bucketizaciÃ³n)  
âœ… Guardar histÃ³rico en **CSV y Parquet**  
âœ… Cargar datos procesados en **Amazon Redshift**  
âœ… Orquestar todo con **Apache Airflow** en Docker  
âœ… ValidaciÃ³n con **tests unitarios** + **CI en GitHub Actions**

---

# ğŸ—ï¸ 2. Arquitectura del Pipeline


            +----------------+
            |  yfinance API  |
            +--------+-------+
                     |
                     v
         +-----------------------+
         |  ExtracciÃ³n (Python)  |
         |  get_stock_data()     |
         +-----------+-----------+
                     |
                     v
      +--------------------------------+
      | TransformaciÃ³n (Python)        |
      | transform_stock_data()         |
      | - Normaliza Ticker             |
      | - Price float â†’ int            |
      | - Price_Bucket (categorÃ­as)    |
      +--------------+-----------------+
                     |
                     v
     +--------------------------------------+
     | Persistencia local                   |
     | - CSV histÃ³rico                      |
     | - Parquet (data/staging)             |
     +----------------+---------------------+
                     |
                     v
         +------------------------------+
         |   Carga a Redshift (Python)  |
         | load_parquet_to_redshift()   |
         +--------------+---------------+
                     |
                     v
            +-------------------+
            |   Data Warehouse  |
            |     Redshift      |
            +-------------------+


---

# ğŸ”§ 3. Transformaciones aplicadas

La funciÃ³n `transform_stock_data(df)` realiza:

### âœ” NormalizaciÃ³n  
- `Ticker` â†’ siempre en **mayÃºsculas**

### âœ” Mantiene tipos nativos  
- `Price` â†’ **float**, sin convertir a int en el pipeline

### âœ” Nueva columna: `Price_Bucket`  
SegÃºn el valor:

| Rango | CategorÃ­a |
|-------|-----------|
| `â‰¤ 100` | LOW |
| `100â€“500` | MEDIUM |
| `> 500` | HIGH |

### âœ” Persistencia
- Se eliminan duplicados por *Date + Ticker*
- Se genera **CSV** y **Parquet**

---

# ğŸ§ª 4. Tests unitarios

UbicaciÃ³n:

tests/test_get_stock_data.py

Los tests cubren:

### âœ” Mock de yfinance  
- Simula la API  
- Evita llamadas reales

### âœ” ValidaciÃ³n de estructura de DataFrame

### âœ” ConversiÃ³n a int (solo test)
- El pipeline mantiene float  
- El test asegura que puede convertirse si se necesitara

### âœ” Prueba de transformaciones:
- Uppercase de `Ticker`
- `Price` es float
- BucketizaciÃ³n correcta

Ejecutar tests:


pytest -q

# ğŸ—„ï¸ 5. Carga a Redshift

Archivo:
app/src/redshift_loader.py

Hace:

Lee Parquet desde data/staging/

Construye motor SQLAlchemy con credenciales desde .env

Inserta los datos en Redshift usando to_sql()

Variables necesarias (.env):

```
REDSHIFT_HOST=
REDSHIFT_PORT=5439
REDSHIFT_USER=
REDSHIFT_PASSWORD=
REDSHIFT_DB=
REDSHIFT_SCHEMA=public
REDSHIFT_TABLE=stock_prices_history
PARQUET_PATH=data/staging/stock_prices_history.parquet
```
âš  .env no debe ser committeado.
Usar .env.example como plantilla.

# ğŸŒ¬ï¸ 6. DAG de Airflow

Ruta:
dags/stocks_redshift_daily_dag.py

Tareas:

1ï¸âƒ£ fetch_stocks_daily

Extrae precios

Ajusta fecha con data_interval_start

Aplica transformaciones

Actualiza CSV + Parquet

2ï¸âƒ£ load_to_redshift

Lee parquet

Carga datos a Redshift

Flujo:

fetch_stocks_daily >> load_to_redshift


# ğŸ“‚ 7. Estructura del proyecto


```
Api_app_Stocks/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ constants.py
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ get_data.py
â”‚       â”œâ”€â”€ redshift_loader.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ stocks_redshift_daily_dag.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_get_stock_data.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ .github/workflows/tests.yml
â”œâ”€â”€ Dockerfile.airflow
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ .env.example
â””â”€â”€ pyproject.toml
```

# ğŸ’» 8. EjecuciÃ³n local (sin Docker)

```
uv sync
uv run python -m app.src.get_data
uv run python -m app.src.redshift_loader
```

# ğŸ³ 9. EjecuciÃ³n con Docker + Airflow

ğŸ”¹ 9.1 Crear .env

cp .env.example .env
Completar valores.

ğŸ”¹ 9.2 Comandos con Makefile (recomendado)

```
make airflow-build
make airflow-init
make airflow-create-user
make airflow-up
```

Airflow UI:
ğŸ‘‰ http://localhost:8080

Usuario: admin
Password: admin

ğŸ”¹ 9.3 Sin Makefile

docker compose build
docker compose up airflow-init
docker compose run --rm airflow-webserver airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com \
  --password admin
docker compose up

Activar DAG â†’ â€œTrigger DAGâ€ â†’ Ver logs.

# ğŸ” 10. Manejo de credenciales



Crear un archivo .env en la raiz:

```
Api_app_Stocks/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ constants.py
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ get_data.py
â”‚       â”œâ”€â”€ redshift_loader.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ stocks_redshift_daily_dag.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_get_stock_data.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ .github/workflows/tests.yml
â”œâ”€â”€ Dockerfile.airflow
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ .env.example
â”œâ”€â”€ .env
â””â”€â”€ pyproject.toml
```

Se debe tomar como plantilla el archivo .env.example. Pegarlo en el archivo .env y completar las credenciales.

.env debe estar en .gitignore

# âœ” 11. Resumen general

```
pytest -q
uv sync
make airflow-build
make airflow-init
make airflow-create-user
make airflow-up
make airflow-down
make airflow-reset
```

