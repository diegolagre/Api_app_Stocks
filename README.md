![CI Status](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml/badge.svg)](https://github.com/diegolagre/Api_app_Stocks/actions/workflows/tests.yml)

# API App Stocks â€“ Data Pipeline con Python, Airflow y Redshift

## ğŸ“Œ DescripciÃ³n general

Este proyecto implementa un pipeline de datos que:

- Obtiene diariamente precios de acciones desde la API de Yahoo Finance (`yfinance`).
- Aplica transformaciones explÃ­citas sobre los datos (normalizaciÃ³n, casting y categorizaciÃ³n).
- Guarda el histÃ³rico en formato **CSV** y **Parquet**.
- Carga los datos transformados a **Amazon Redshift** usando SQLAlchemy.
- Orquesta todo con **Apache Airflow** corriendo en Docker.
- Asegura calidad con **tests unitarios** y **CI en GitHub Actions**.

---

## ğŸ—ï¸ Arquitectura del pipeline

```text
            +----------------+
            |  yfinance API  |
            +--------+-------+
                     |
                     v
         +-----------------------+
         |  ExtracciÃ³n (Python)  |
         |  get_stock_data()     |
         +-----------+-----------+
                     |
                     v
      +--------------------------------+
      | TransformaciÃ³n (Python)        |
      | transform_stock_data()         |
      | - Normaliza Ticker             |
      | - Price float â†’ int            |
      | - Price_Bucket (categorÃ­as)    |
      +--------------+-----------------+
                     |
                     v
     +--------------------------------------+
     | Persistencia local                   |
     | - CSV histÃ³rico                      |
     | - Parquet (data/staging)             |
     +----------------+---------------------+
                     |
                     v
         +------------------------------+
         |   Carga a Redshift (Python)  |
         | load_parquet_to_redshift()   |
         +--------------+---------------+
                     |
                     v
            +-------------------+
            |   Data Warehouse  |
            |     Redshift      |
            +-------------------+

---

## ğŸ”§ Transformaciones de datos

La funciÃ³n `transform_stock_data(df)` aplica transformaciones de negocio sobre el DataFrame de precios:

- **NormalizaciÃ³n**
  - `Ticker` â†’ convertido a mayÃºsculas (`AAPL`, `NVDA`, `GOOGL`, etc.).
- **Casting de tipos**
  - `Price` â†’ convertido a entero (`int`) usando casting seguro.
- **Columna derivada**
  - `Price_Bucket` segÃºn el rango de precio:
    - `LOW`    â†’ precios â‰¤ 100  
    - `MEDIUM` â†’ 100 < precio â‰¤ 500  
    - `HIGH`   â†’ precio > 500  

Adicionalmente:

- EliminaciÃ³n de duplicados en el histÃ³rico por (`Date`, `Ticker`).
- Persistencia en CSV + Parquet.

---

## ğŸ§ª Tests unitarios

Los tests incluidos verifican:

- Uso de mocks para yfinance (sin llamar a la API real).
- ConversiÃ³n correcta de precios float â†’ int.
- Integridad de columnas.
- ComprobaciÃ³n de transformaciones (`Ticker`, `Price`, `Price_Bucket`).

Ejecutarlos:

```
pytest -q
```

---

## ğŸ—„ï¸ Carga a Redshift

`redshift_loader.py`:

- Lee credenciales desde `.env`.
- Crea engine SQLAlchemy.
- Lee Parquet: `data/staging/stock_prices_history.parquet`.
- Ejecuta `to_sql()` hacia Redshift.

Variables necesarias:

```
REDSHIFT_HOST=
REDSHIFT_PORT=
REDSHIFT_USER=
REDSHIFT_PASSWORD=
REDSHIFT_DB=
REDSHIFT_SCHEMA=
REDSHIFT_TABLE=
PARQUET_PATH=data/staging/stock_prices_history.parquet
```

---

## ğŸŒ¬ï¸ DAG de Airflow

`dags/stocks_redshift_daily_dag.py`

Tareas:

1. `fetch_stocks_daily`
2. `load_to_redshift`

Flujo:

```
fetch_stocks_daily >> load_to_redshift
```

Escribe CSV y Parquet en:

```
data/stock_prices_history.csv
data/staging/stock_prices_history.parquet
```

---

## ğŸ“‚ Estructura del proyecto

```
Api_app_Stocks/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ constants.py
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ get_data.py
â”‚       â”œâ”€â”€ redshift_loader.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ stocks_redshift_daily_dag.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_get_stock_data.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ tests.yml
â”œâ”€â”€ Dockerfile.airflow
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ’» EjecuciÃ³n local (Python)

```
uv sync
uv run python -m app.src.get_data
uv run python -m app.src.redshift_loader
```

---

## ğŸ³ EjecuciÃ³n con Docker + Airflow

### 1. Crear archivo `.env`

```
cp .env.example .env
```

### 2. Construir la imagen:

```
docker compose build
```

### 3. Inicializar Airflow:

```
docker compose up airflow-init
```

### 4. Crear usuario admin:

```
docker compose run --rm airflow-webserver airflow users create   --username admin   --firstname Admin   --lastname User   --role Admin   --email admin@example.com   --password admin
```

### 5. Levantar Airflow:

```
docker compose up
```

UI: http://localhost:8080

Login: `admin / admin`

### 6. Activar y correr el DAG

1. Activar toggle del DAG  
2. "Trigger DAG"  
3. Revisar logs de `fetch_stocks_daily` y `load_to_redshift`.

---

## ğŸ” Manejo de credenciales

`.env` debe estar en `.gitignore`  
`.env.example` solo contiene placeholders.

---

## âœ… Resumen de comandos

```
pytest -q
uv sync
docker compose build
docker compose up airflow-init
docker compose run --rm airflow-webserver airflow users create ...
docker compose up
docker compose down
```

